
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic</title>

    <meta name="description" content="Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-52J0PM8XKV');
</script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Enhancing Zero-Shot Chain-of-Thought Reasoning <br>
                in Large Language Models through Logic
            </h2>
		            <div class="col-md-12 text-center">    
		(Arxiv Preprint)
				     </div>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                <br>
                <li><a href="https://xf-zhao.github.io">Xufeng Zhao</a> </li> 
                <li><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/lu.html">Wenhao Lu</a> </li> 
		<li><a href="https://mengdi-li.github.io/">Mengdi Li</a></li> 
		<li><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/weber.html">Cornelius Weber</a></li> 
        <br>
		<li><a href="https://jaeheelee.gitlab.io/">Jae Hee Lee</a></li> 
		<li><a href="https://kchu.github.io/">Kun Chu</a></li> 
		<li><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/wermter.html">Stefan Wermter</a></li>
                <br><br>
                    <a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm.html">
                    <image src="img/new-wtm-logo-white-150x150.jpg" height="40px"> Knowledge Technology Group</a>
                </ul>
            </div>
        </div>

        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2309.13339">
                            <image src="img/logicot_small.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                         <li>
                            <a href="">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li> 
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="text-center">
                    <img src="/img/LogiCoT_preview.gif">
                </div>
                <h3>
Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning.
Although large language models possess extensive knowledge, their behavior, particularly in terms of reasoning, often fails to effectively utilize this knowledge to establish a coherent thinking paradigm.
Generative language models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles.
Aiming to improve the zero-shot chain-of-thought reasoning ability of large language models, we propose Logical Chain-of-Thought (LogiCoT), a neurosymbolic framework that leverages principles from symbolic logic to verify and revise the reasoning processes accordingly.
Experimental evaluations conducted on language tasks in diverse domains, including arithmetic, commonsense, symbolic, causal inference, and social problems, demonstrate the efficacy of the enhanced reasoning paradigm by logic.
                </h3>
                <p class="text-justify">
                    Abstract here.
                </p>
            </div>
        </div>


         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation 
                </h3> <a href="">[arxiv version]</a>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@misc{Zhao23EnhancingZeroShot,
  title = {Enhancing {{Zero-Shot Chain-of-Thought Reasoning}} in {{Large Language Models}} through {{Logic}}},
  author = {Zhao, Xufeng and Li, Mengdi and Lu, Wenhao and Weber, Cornelius and Lee, Jae Hee and Chu, Kun and Wermter, Stefan},
  year = {2023},
  month = sep,
  number = {arXiv:2309.13339},
  eprint = {2309.13339},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.13339},
  urldate = {2023-09-26},
  abstract = {Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their behavior, particularly in terms of reasoning, often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. Generative language models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming to improve the zero-shot chain-of-thought reasoning ability of large language models, we propose Logical Chain-of-Thought (LogiCoT), a neurosymbolic framework that leverages principles from symbolic logic to verify and revise the reasoning processes accordingly. Experimental evaluations conducted on language tasks in diverse domains, including arithmetic, commonsense, symbolic, causal inference, and social problems, demonstrate the efficacy of the enhanced reasoning paradigm by logic.},
  archiveprefix = {arxiv},
  arxiv = {2309.13339},
}
                    </textarea>
                </div>
            </div>
        </div>


    </div>
</body>
</html>
